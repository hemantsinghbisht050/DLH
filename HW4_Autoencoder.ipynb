{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:14.326188Z",
     "start_time": "2023-02-28T02:57:13.355473Z"
    },
    "id": "QWbsirOOvRq9",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c64c6304db7ccbe2c068528004c398d2",
     "grade": false,
     "grade_id": "cell-ad33073e8b0cba4e",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "### Import all the libraries used\n",
    "import os\n",
    "import csv\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "### Set random seed\n",
    "seed = 24\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "\n",
    "\n",
    "# record start time\n",
    "_START_RUNTIME = time.time()\n",
    "\n",
    "# Define data and weight path\n",
    "DATA_PATH = \"../HW4_Autoencoder-lib/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0be6ca11d51ca9b4a8017f78e04e0165",
     "grade": false,
     "grade_id": "cell-115765ba096f745e",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1 Load and Visualize the Data [10 points]\n",
    "\n",
    "The data is under `DATA_PATH`. In this part, you are required to load the data into the data loader, and calculate some statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:14.335735Z",
     "start_time": "2023-02-28T02:57:14.328277Z"
    },
    "deletable": false
   },
   "outputs": [],
   "source": [
    "#input\n",
    "# folder: str, 'train', 'val', or 'test'\n",
    "#output\n",
    "# number_normal: number of normal samples in the given folder\n",
    "# number_pneumonia: number of pneumonia samples in the given folder\n",
    "def get_count_metrics(folder, data_path=DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the number of normal and pneumonia samples.\n",
    "          Hint: !ls $DATA_PATH\n",
    "    '''\n",
    "    normal_path = os.path.join(data_path, folder, 'NORMAL')\n",
    "    pneumonia_path = os.path.join(data_path, folder, 'PNEUMONIA')\n",
    "    number_normal = len(os.listdir(normal_path))\n",
    "    number_pneumonia = len(os.listdir(pneumonia_path))\n",
    "    # your code here\n",
    "#     raise NotImplementedError\n",
    "\n",
    "    return number_normal, number_pneumonia\n",
    "\n",
    "\n",
    "#output\n",
    "# train_loader: train data loader (type: torch.utils.data.DataLoader)\n",
    "# val_loader: val data loader (type: torch.utils.data.DataLoader)\n",
    "def load_data(data_path=DATA_PATH):\n",
    "    \n",
    "    '''\n",
    "    TODO: Implement this function to return the data loader for \n",
    "    train and validation dataset. Set batchsize to 32.\n",
    "    \n",
    "    You should add the following transforms (https://pytorch.org/docs/stable/torchvision/transforms.html):\n",
    "        1. transforms.RandomResizedCrop: the images should be cropped to 224 x 224\n",
    "        2. transforms.RandomResizedCrop: the images should be compressed to 24 x 24\n",
    "        3. transforms.ToTensor: just to convert data/labels to tensors\n",
    "        4. flatten_transform: to flatten the images away from their 3 x 24 x 24 representation (provided)\n",
    "    You should set the *shuffle* flag for *train_loader* to be True, and False for *val_loader*.\n",
    "    \n",
    "    HINT: Consider using `torchvision.datasets.ImageFolder`.\n",
    "    '''\n",
    "\n",
    "    import torchvision\n",
    "    import torchvision.datasets as datasets\n",
    "    import torchvision.transforms as transforms\n",
    "\n",
    "    flatten_transform = transforms.Lambda(lambda x: torch.flatten(x))\n",
    "    # your code here\n",
    "    transform = transforms.Compose([transforms.RandomResizedCrop(224),transforms.Resize(24),transforms.ToTensor(),flatten_transform])\n",
    "    train_data = datasets.ImageFolder(os.path.join(data_path, 'train'), transform=transform)\n",
    "    val_data = datasets.ImageFolder(os.path.join(data_path, 'val'), transform=transform)\n",
    "    train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_data, batch_size=32, shuffle=False)\n",
    "#     raise NotImplementedError\n",
    "    \n",
    "    return train_loader, val_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:16.308379Z",
     "start_time": "2023-02-28T02:57:14.553189Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a555836ab06dfbb3345ff594bb01a5b8",
     "grade": false,
     "grade_id": "cell-e908531cdc33dff4",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# DO NOT MODIFY THIS PART\n",
    "\n",
    "\n",
    "import torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def imshow(img, title):\n",
    "    npimg = img.numpy()\n",
    "    plt.figure(figsize=(15, 7))\n",
    "    plt.axis('off')\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "def show_batch_images(dataloader, k=8):\n",
    "    images, labels = next(iter(dataloader))\n",
    "    images = images.reshape(-1, 3, 24, 24)\n",
    "    images = images[:k]\n",
    "    labels = labels[:k]\n",
    "    img = torchvision.utils.make_grid(images, padding=3)\n",
    "    imshow(img, title=[\"NORMAL\" if x==0  else \"PNEUMONIA\" for x in labels])\n",
    "\n",
    "train_loader, val_loader = load_data()   \n",
    "for i in range(2):\n",
    "    show_batch_images(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vYh25HwvRq7",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e0289a6c9f72f08e66d0b183e6364503",
     "grade": false,
     "grade_id": "cell-25ebf45813d4a47b",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:16.319632Z",
     "start_time": "2023-02-28T02:57:16.310451Z"
    },
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the MLP shown above.\n",
    "HINT: Consider using `nn.Linear`, `torch.relu`, and `torch.sigmoid`.\n",
    "\"\"\"\n",
    "\n",
    "class VanillaAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VanillaAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.fc1 = nn.Linear(1728, 128)\n",
    "        self.fc2 = nn.Linear(128, 16)\n",
    "        self.fc3 = nn.Linear(16, 128)\n",
    "        self.fc4 = nn.Linear(128, 1728)\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize the model layers as shown above.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "#         raise NotImplementedError\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform encoding operation with fc1, fc2, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def decode(self, x):\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decode(self.encode(x))  \n",
    "\n",
    "# initialize the NN\n",
    "model = VanillaAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.075697Z",
     "start_time": "2023-02-28T02:57:17.065689Z"
    },
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the MLP shown above.\n",
    "HINT: Consider using `nn.Linear` and `torch.sigmoid`.\n",
    "\"\"\"\n",
    "\n",
    "class SparseAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize the model layers as shown above.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "        # used in training as sparsity regularization\n",
    "        self.data_rho = 0\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform encoding operation with fc1, fc2, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def decode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform decoding operation with fc3, fc4, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encode(x)\n",
    "        self.data_rho = x.mean(0)\n",
    "        x = self.decode(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "# initialize the NN\n",
    "model = SparseAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:17.679981Z",
     "start_time": "2023-02-28T02:57:17.668553Z"
    },
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the MLP shown above.\n",
    "HINT: Consider using `nn.Linear`, `torch.relu`, and `torch.sigmoid`.\n",
    "\"\"\"\n",
    "\n",
    "class DenoisingAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DenoisingAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.fc1 = None\n",
    "        self.fc2 = None\n",
    "        self.fc3 = None\n",
    "        self.fc4 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize the model layers as shown above.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: Perform encoding operation with fc1, fc2, and the corresponding activation function.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def decode(self, x):\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        noise = None\n",
    "        std = 0.1\n",
    "        mean = 0\n",
    "        \"\"\"\n",
    "        TODO: Generate the noise from the normal distribution with the above mean and std.\n",
    "        \n",
    "        Note that the size of the noise should be the same as x.\n",
    "        \n",
    "        Hint: Use torch.randn().\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        x = x + noise\n",
    "        return self.decode(self.encode(x))  \n",
    "\n",
    "# initialize the NN\n",
    "model = DenoisingAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:19.117415Z",
     "start_time": "2023-02-28T02:57:19.100310Z"
    },
    "id": "k_eVdpPJvRrR",
    "outputId": "a6de63b9-0306-4307-e41e-ec44ced7cbaf"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Build the StackedAutoencoder using your VanillaAutoencoder architecture.\n",
    "\"\"\"\n",
    "\n",
    "class StackedAutoencoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(StackedAutoencoder, self).__init__()\n",
    "        \n",
    "        # DO NOT change the names\n",
    "        self.ae1 = None\n",
    "        self.ae2 = None\n",
    "        self.ae3 = None\n",
    "        \n",
    "        \"\"\"\n",
    "        TODO: Initialize three Vanilla Autoencoders and assign them to self.ae1, self.ae2, self.ae3, respectively.\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.ae1(x)\n",
    "        x = self.ae2(x)\n",
    "        x = self.ae3(x)\n",
    "        return x\n",
    "        \n",
    "    def encode(self, x):\n",
    "        \"\"\"\n",
    "        TODO: While we didn't implement the forward() function of the\n",
    "        StackedAutoencoder as using an encode() and decode() function, \n",
    "        we may still be interested in the future of extracting the \n",
    "        compressed representation. So, implement the encode function\n",
    "        to return the compressed representation from the third\n",
    "        VanillaAutoencoder component (note you will have to call its \n",
    "        encode function).\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "\n",
    "# initialize the NN\n",
    "model = StackedAutoencoder()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.488662Z",
     "start_time": "2023-02-28T02:57:20.486044Z"
    },
    "id": "TMHiaMn4vRrV"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Define the loss (MSELoss), assign it to `criterion`.\n",
    "\n",
    "REFERENCE: https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html#torch.nn.MSELoss\n",
    "\"\"\"\n",
    "\n",
    "criterion = None\n",
    "\n",
    "# your code here\n",
    "raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:20.916940Z",
     "start_time": "2023-02-28T02:57:20.501045Z"
    },
    "id": "EOjEFit_vRrX"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import *\n",
    "\n",
    "#input: Y_pred,Y_true\n",
    "#output: mean squared error, mean absolute error\n",
    "def classification_metrics(X_reconstructed, X_original):\n",
    "    mse, mae = mean_squared_error(X_original, X_reconstructed), \\\n",
    "               mean_absolute_error(X_original, X_reconstructed)\n",
    "    return mse, mae\n",
    "\n",
    "\n",
    "\n",
    "#input: model, loader\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    all_X_original = torch.FloatTensor()\n",
    "    all_X_reconstructed = torch.FloatTensor()\n",
    "    for x, _ in loader:\n",
    "        x_reconstructed = model(x)\n",
    "        \"\"\"\n",
    "        TODO: Add the correct values to the lists in order to keep a\n",
    "        running tab of all of the original and reconstructed inputs.\n",
    "        \n",
    "        Hint: use torch.cat().\n",
    "        \"\"\"\n",
    "        # your code here\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    mse, mae = classification_metrics(all_X_reconstructed.detach().numpy(), all_X_original.detach().numpy())\n",
    "    print(f\"mse: {mse:.3f}, mae: {mae:.3f}\")\n",
    "    return mse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.965590Z",
     "start_time": "2023-02-28T02:57:20.918604Z"
    },
    "id": "L9HTWrsMvRrX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "08e13f134270ece398235bbfbe59f555",
     "grade": false,
     "grade_id": "cell-8970a0bdbd7b14bb",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "outputId": "d31e0567-e8eb-4502-df19-c9e1317358ed"
   },
   "outputs": [],
   "source": [
    "print(\"model perfomance before training:\")\n",
    "# initialized the model\n",
    "model = VanillaAutoencoder()\n",
    "mae_train_init = evaluate(model, train_loader)[1]\n",
    "mae_val_init = evaluate(model, val_loader)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.976335Z",
     "start_time": "2023-02-28T02:57:36.973144Z"
    },
    "id": "Rq5Ov7PsvRrW"
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TODO: Define the optimizer (Adam) with learning rate 0.001, assign it to `optimizer`.\n",
    "\n",
    "REFERENCE: https://pytorch.org/docs/stable/optim.html\n",
    "\"\"\"\n",
    "def get_optimizer(model):\n",
    "    optimizer = None\n",
    "\n",
    "    # your code here\n",
    "    raise NotImplementedError\n",
    "   \n",
    "    return optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:57:36.999120Z",
     "start_time": "2023-02-28T02:57:36.992353Z"
    },
    "id": "TJKLcG1svRrY",
    "outputId": "5163b996-446e-4e5a-e23f-9d4845230747"
   },
   "outputs": [],
   "source": [
    "def train_model(model):\n",
    "    # number of epochs to train the model\n",
    "    n_epochs = 10\n",
    "    \n",
    "    # get the correct type of optimizer for the model\n",
    "    optimizer = get_optimizer(model)\n",
    "\n",
    "    # prep model for training\n",
    "    model.train()\n",
    "\n",
    "    train_loss_arr = []\n",
    "    for epoch in range(n_epochs):\n",
    "\n",
    "        train_loss = 0\n",
    "        for x, _ in train_loader:\n",
    "            \"\"\" Step 1. clear gradients \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            \"\"\" \n",
    "            TODO: Step 2. perform forward pass using `model`, save the output to x_reconstructed;\n",
    "                  Step 3. calculate the loss using `criterion`, save the output to loss.\n",
    "                      If the model is a SparseAutoencoder, the loss will have an additional\n",
    "                      regularization penalty. This is calculated by:\n",
    "                          average of (- rho * log(data_rho)  +  (1 - rho) * log(1 - data_rho))\n",
    "                      where we will use rho of 0.1\n",
    "            \"\"\"\n",
    "            x_reconstructed = None\n",
    "            loss = None\n",
    "            # your code here\n",
    "            raise NotImplementedError\n",
    "            if isinstance(model, SparseAutoencoder):\n",
    "                penalty = None\n",
    "                rho = 0.1\n",
    "                data_rho = model.data_rho\n",
    "                # your code here\n",
    "                raise NotImplementedError\n",
    "                loss = loss + (0.5 * penalty)\n",
    "            \"\"\" Step 4. backward pass \"\"\"\n",
    "            loss.backward()\n",
    "            \"\"\" Step 5. optimization \"\"\"\n",
    "            optimizer.step()\n",
    "            \"\"\" Step 6. record loss \"\"\"\n",
    "            train_loss += loss.item()\n",
    "\n",
    "        train_loss = train_loss / len(train_loader)\n",
    "        if epoch % 2 == 0:\n",
    "            train_loss_arr.append(np.mean(train_loss))\n",
    "            print('Epoch: {} \\tTraining Loss: {:.6f}'.format(epoch, train_loss))\n",
    "            evaluate(model, val_loader)\n",
    "            \n",
    "    return model, train_loss_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T02:59:58.010928Z",
     "start_time": "2023-02-28T02:57:37.000785Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9b46516921893f6bcb901824ada47e5f",
     "grade": false,
     "grade_id": "cell-09440f46b9f76e9f",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "vanilla_model = VanillaAutoencoder()\n",
    "vanilla_model, vanilla_train_loss_arr = train_model(vanilla_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:02:00.856833Z",
     "start_time": "2023-02-28T02:59:58.012335Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a5feff9e0b0d6accf7ac9d3a0c966918",
     "grade": false,
     "grade_id": "cell-c3305e09c12ff3d8",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "sparse_model = SparseAutoencoder()\n",
    "sparse_model, sparse_train_loss_arr = train_model(sparse_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:04:16.248451Z",
     "start_time": "2023-02-28T03:02:00.858105Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3701136cc45abf4a32af7ea1b4ba898d",
     "grade": false,
     "grade_id": "cell-d4b24af37b1e2e39",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "denoising_model = DenoisingAutoencoder()\n",
    "denoising_model, denoising_train_loss_arr = train_model(denoising_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-02-28T03:06:31.256159Z",
     "start_time": "2023-02-28T03:04:16.249609Z"
    },
    "nbgrader": {
     "cell_type": "code",
     "checksum": "862507eee2ce90717680cc54b5bdf975",
     "grade": false,
     "grade_id": "cell-9e3c3ee59a65657b",
     "locked": false,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "stacked_model = StackedAutoencoder()\n",
    "stacked_model, stacked_train_loss_arr = train_model(stacked_model)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "HILSnC7XvRrE",
    "jJ4mvmTavRrL",
    "0lnxAWWnvRrQ",
    "j1MoUmqmvRrU"
   ],
   "name": "HW2_NN_pre.ipynb",
   "provenance": []
  },
  "illinois_payload": {
   "b64z": "",
   "nb_path": "release/HW4_Autoencoder/HW4_Autoencoder.ipynb"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "398.390625px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "524px",
    "left": "1423px",
    "right": "20px",
    "top": "120px",
    "width": "348px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
